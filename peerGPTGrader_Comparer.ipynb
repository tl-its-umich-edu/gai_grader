{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-11T10:57:00-0400 INFO [helper.py:417] - All configuration parameters set up successfully.\n",
      "2023-07-11T10:57:02-0400 INFO [helper.py:417] - All configuration parameters set up successfully.\n",
      "2023-07-11T10:57:04-0400 INFO [helper.py:417] - All configuration parameters set up successfully.\n"
     ]
    }
   ],
   "source": [
    "from helper import *\n",
    "from postProcessHelper import *\n",
    "\n",
    "varsDictP1 = setUpPostProcessVars(versionControl='V3', promptVersion='P1', simpleCourseName='MOVESCI')\n",
    "varsDictP2 = setUpPostProcessVars(versionControl='V3', promptVersion='P2', simpleCourseName='MOVESCI')\n",
    "\n",
    "resultsDFCols = ['submitter_id', 'grader_id', 'rubric_id', 'assignment_id', 'score', 'points_possible']\n",
    "\n",
    "config = Config()\n",
    "config.setFromEnv()\n",
    "\n",
    "versionControl = 'V3'\n",
    "promptVersion='P2'\n",
    "config.simpleCourseName = 'MOVESCI'\n",
    "fullName = f'{config.simpleCourseName}-{versionControl}-{promptVersion}'\n",
    "\n",
    "config.setSaveDetails(fullName)\n",
    "\n",
    "excelFolder = os.path.join(config.baseOutputFolder, \\\n",
    "                           config.outputFolders['EXCEL_OUTPUT'], \\\n",
    "                           config.fullName)\n",
    "\n",
    "compareResultsDF = varsDictP1['resultsDF'].merge(varsDictP2['resultsDF'], \\\n",
    "                                                 on=resultsDFCols, how='inner', \\\n",
    "                                                 suffixes=('_P1', '_P2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullInfoDF, rubricOrderDict = buildComparerFullInfoDF(config, compareResultsDF, saveName, excelFolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['11',\n",
       " '11',\n",
       " '10',\n",
       " '11',\n",
       " '00',\n",
       " '00',\n",
       " '00',\n",
       " '00',\n",
       " '00',\n",
       " '00',\n",
       " '00',\n",
       " '11',\n",
       " '00',\n",
       " '00',\n",
       " '00',\n",
       " '11',\n",
       " '00',\n",
       " '00',\n",
       " '00',\n",
       " '00',\n",
       " '01',\n",
       " '00',\n",
       " '00',\n",
       " '10',\n",
       " '11',\n",
       " '10',\n",
       " '11',\n",
       " '11',\n",
       " '11',\n",
       " '00',\n",
       " '00',\n",
       " '11',\n",
       " '00',\n",
       " '00',\n",
       " '00',\n",
       " '00']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getHighErrorCriteria(config, fullInfoDF, saveName, excelFolder, scoreThreshold = 10):\n",
    "    pd.set_option('display.precision', 2)\n",
    "\n",
    "    descDataDF = config.critDescDF[['custom_description', 'assignment_id', 'id']]\n",
    "    criteriaIssuesDF = pd.DataFrame()\n",
    "    P1Count,P2Count = 0,0\n",
    "    binaryTracker = []\n",
    "    for AID in fullInfoDF['assignment_id'].unique():\n",
    "        for CID in fullInfoDF[(fullInfoDF['assignment_id']==AID)]['criterion_id'].unique(): \n",
    "            subsetDF =  fullInfoDF[(fullInfoDF['assignment_id']==AID) \\\n",
    "                                   & (fullInfoDF['criterion_id']==CID)]\n",
    "            if subsetDF['Mean Difference % P1'].mean() > scoreThreshold:\n",
    "                issueDF = subsetDF[['assignment_id', 'criterion_id', \\\n",
    "                                    'assignment_title', 'description_rubric', \\\n",
    "                                    'points_rubric', 'All Graders Mean', \\\n",
    "                                    'Global peerGPT Mean P1']].drop_duplicates()\n",
    "                issueDF['Mean Difference % P1'] = subsetDF['Mean Difference % P1'].mean()\n",
    "                P1Count += 1\n",
    "\n",
    "            if subsetDF['Mean Difference % P2'].mean() > scoreThreshold:\n",
    "                issueDF = subsetDF[['assignment_id', 'criterion_id', \\\n",
    "                                    'assignment_title', 'description_rubric', \\\n",
    "                                    'points_rubric', 'All Graders Mean', \\\n",
    "                                    'Global peerGPT Mean P2']].drop_duplicates()\n",
    "                issueDF['Mean Difference % P2'] = subsetDF['Mean Difference % P2'].mean()\n",
    "                P2Count += 1\n",
    "            \n",
    "            binaryTracker.append(str(int(subsetDF['Mean Difference % P1'].mean() > scoreThreshold))\\\n",
    "                               + str(int(subsetDF['Mean Difference % P2'].mean() > scoreThreshold)))\n",
    "            \n",
    "            criteriaIssuesDF = pd.concat([criteriaIssuesDF, issueDF])\n",
    "\n",
    "    criteriaIssuesDF = criteriaIssuesDF.merge(descDataDF, left_on=['assignment_id', 'criterion_id'], \\\n",
    "                                            right_on=['assignment_id', 'id'])\n",
    "    del criteriaIssuesDF['id']\n",
    "    criteriaIssuesDF = criteriaIssuesDF.rename(columns={'assignment_title':'Title', \\\n",
    "                                                        'custom_description':'Supplemental Description', \\\n",
    "                                                        'description_rubric':'Rubric', \\\n",
    "                                                        'points_rubric':'Max Score'}).reset_index(drop=True)\n",
    "    criteriaIssuesDF.to_excel(os.path.join(excelFolder, saveName+' - High Error Criteria.xlsx'))\n",
    "\n",
    "    return criteriaIssuesDF, {'With no guide':P1Count, 'With grading guide':P2Count}, binaryTracker\n",
    "\n",
    "criteriaIssuesDF, hitCounts, binaryTracker = getHighErrorCriteria(config, fullInfoDF, saveName, excelFolder, scoreThreshold = 10)\n",
    "\n",
    "binaryTracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
