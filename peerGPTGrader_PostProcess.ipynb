{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import *\n",
    "from postProcessHelper import *\n",
    "\n",
    "config = Config()\n",
    "config.setFromEnv()\n",
    "\n",
    "versionControl = 'V3'\n",
    "promptVersion='P3'\n",
    "simpleCourseName = 'ECON'\n",
    "fullName = f'{simpleCourseName}-{versionControl}-{promptVersion}'\n",
    "config.customDescMode = False\n",
    "config.setSaveDetails(fullName)\n",
    "\n",
    "chartFolder = os.path.join(config.baseOutputFolder, \\\n",
    "                           config.outputFolders['CHART_OUTPUT'], \\\n",
    "                           config.fullName)\n",
    "\n",
    "excelFolder = os.path.join(config.baseOutputFolder, \\\n",
    "                           config.outputFolders['EXCEL_OUTPUT'], \\\n",
    "                           config.fullName)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultsDF = convertPicklesToDF('saves', config)\n",
    "errorDF = convertPicklesToDF('errors', config)\n",
    "\n",
    "mergedCriterionData = getCriterionDataDF(resultsDF, fullName, excelFolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getScoreSpread(resultsDF, chartFolder)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Track Difference in scores per Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveGraderPeerGPTMeanScoreDiff(resultsDF, saveName, excelFolder):\n",
    "    excludeDF = resultsDF.copy()\n",
    "    excludeDF['Score Difference'] = excludeDF['peerGPT_score']-excludeDF['score']\n",
    "    excludeDF['Score Diff. %'] = 100*(excludeDF['peerGPT_score']-excludeDF['score'])/excludeDF['points_possible']\n",
    "\n",
    "    meanDiffDict = {}\n",
    "    meanDiffPercentDict = {}\n",
    "    for group in excludeDF.groupby(['grader_id','assignment_id']):\n",
    "        if group[0][0] not in meanDiffDict:\n",
    "            meanDiffDict[group[0][0]] = {}\n",
    "            meanDiffPercentDict[group[0][0]] = {}\n",
    "        meanDiffDict[group[0][0]][group[0][1]] = group[1][\"Score Difference\"].mean()\n",
    "        meanDiffPercentDict[group[0][0]][group[0][1]] = np.round(group[1][\"Score Diff. %\"].mean() , 2)\n",
    "    meanDiffDF = pd.DataFrame(meanDiffDict)\n",
    "    meanDiffPercentDF = pd.DataFrame(meanDiffPercentDict)\n",
    "    meanDiffDF.to_excel(os.path.join(excelFolder, saveName+' - Grader - peerGPT Score Difference.xlsx'))\n",
    "    meanDiffPercentDF.to_excel(os.path.join(excelFolder, saveName+' - Grader - peerGPT Score Diff. %.xlsx'))\n",
    "    return meanDiffPercentDF\n",
    "\n",
    "saveGraderPeerGPTMeanScoreDiff(resultsDF, fullName, excelFolder)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Track difference in score per Criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullInfoDF, rubricOrderDict = buildFullInfoDF(config, resultsDF, fullName, excelFolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getMeanDiffPercentCharts(fullInfoDF, rubricOrderDict, chartFolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meanConDF, retrievedCIDF = getCIOutlierGraderDetails(fullInfoDF, fullName, excelFolder, confidenceLevel=0.99)\n",
    "\n",
    "display(meanConDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getHighErrorCriteria(config, fullInfoDF, fullName, excelFolder, scoreThreshold = 10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Histogram spread of scores by Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getFullHistogramSpread(resultsDF, chartFolder)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Scatterplot spread of scores by Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getFullScatterplotSpread(resultsDF, chartFolder)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Histogram spread of scores by Criterion per Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getHistogramSpreadByAssgn(resultsDF, chartFolder)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Scatterplot spread of scores by Criterion per Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getScatterplotSpreadByAssgn(resultsDF, chartFolder)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Misc. Code Snippets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getMeanDiffCharts(fullInfoDF, rubricOrderDict, chartFolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import *\n",
    "\n",
    "config = Config()\n",
    "config.setFromEnv()\n",
    "config.customDescMode = False\n",
    "\n",
    "versionControl = 'V3'\n",
    "promptVersion='P2'\n",
    "simpleCourseName = 'ECON'\n",
    "fullName = f'{simpleCourseName}-{versionControl}-{promptVersion}'\n",
    "config.setSaveDetails(fullName)\n",
    "\n",
    "gradeRubricAssignmentDF = getGRAData(config)\n",
    "\n",
    "for index, row in gradeRubricAssignmentDF.iterrows():\n",
    "    fullPrompt = processTokenCount(row, config)\n",
    "\n",
    "    if fullPrompt:\n",
    "        with open(os.path.join(config.baseOutputFolder, \\\n",
    "                               config.outputFolders['PROMPT_FILES'], \\\n",
    "                               config.fullName, \\\n",
    "                               f'{config.fullName}_exampleFilledPrompt.txt'), 'w') as textFile:\n",
    "            textFile.write(fullPrompt)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from helper import *\n",
    "\n",
    "config = Config()\n",
    "\n",
    "sourceFolder = os.path.join(config.baseOutputFolder, config.outputFolders['PROMPT_FILES'])\n",
    "destinationFolder = config.promptFolder\n",
    "\n",
    "if os.path.exists(destinationFolder):\n",
    "    shutil.rmtree(destinationFolder)\n",
    "shutil.copytree(sourceFolder, destinationFolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P2DF = pd.read_excel('output\\\\Excel File Outputs\\\\ECON-V3-P2\\\\ECON-V3-P2 - Grader - peerGPT Score Diff. %.xlsx')\n",
    "P3DF = pd.read_excel('output\\\\Excel File Outputs\\\\ECON-V3-P3\\\\ECON-V3-P3 - Grader - peerGPT Score Diff. %.xlsx')\n",
    "\n",
    "display(P2DF)\n",
    "display(P3DF)\n",
    "display(P2DF-P3DF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
