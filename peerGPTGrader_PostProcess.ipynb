{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import *\n",
    "\n",
    "config = Config()\n",
    "config.setFromEnv()\n",
    "\n",
    "versionControl = 'V3'\n",
    "promptVersion='P2'\n",
    "fullName = f'{versionControl}-{promptVersion}'\n",
    "config.setSaveDetails(fullName)\n",
    "\n",
    "saveName = f\"{versionControl}-{promptVersion}\"\n",
    "\n",
    "chartFolder = os.path.join(config.baseOutputFolder, \\\n",
    "                           config.outputFolders['CHART_OUTPUT'], \\\n",
    "                           config.fullName)\n",
    "\n",
    "excelFolder = os.path.join(config.baseOutputFolder, \\\n",
    "                           config.outputFolders['EXCEL_OUTPUT'], \\\n",
    "                           config.fullName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultsDF = convertPicklesToDF('saves', config)\n",
    "errorDF = convertPicklesToDF('errors', config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set_theme(style=\"whitegrid\", palette=\"deep\")\n",
    "\n",
    "filterDF = resultsDF#[resultsDF['assignment_id']!=1916709]\n",
    "sns.jointplot(data=filterDF, x='score', y='peerGPT_score', hue='assignment_id', height=5, marker=\".\", s=50, palette=sns.color_palette()[:6])\n",
    "plt.plot([0,40],[0,40], lw=1, color='#313232', linestyle='dashed')\n",
    "# plt.plot([1,46],[0,40], lw=1, color='#aaaaaa', linestyle='dashed')\n",
    "# plt.plot([0,40],[1,46], lw=1, color='#aaaaaa', linestyle='dashed')g.set_xlabel('Grader Score',fontsize=8)\n",
    "plt.xlabel('Grader Score', fontsize=12)\n",
    "plt.ylabel('peerGPT Score', fontsize=12, rotation=90)\n",
    "plt.legend(title='Asgn. ID', fontsize=8)\n",
    "# plt.show()\n",
    "plt.savefig(os.path.join(chartFolder, 'JointPlot.png'), dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mergedCriterionData = pd.DataFrame()\n",
    "\n",
    "for index,row in resultsDF.iterrows():\n",
    "    criterionData = row['data_peerGPT']\n",
    "    for col in ['submitter_id', 'assignment_id']:\n",
    "        criterionData[col] = row[col]\n",
    "    mergedCriterionData = pd.concat([mergedCriterionData, criterionData])\n",
    "\n",
    "mergedCriterionData.to_excel(os.path.join(excelFolder, saveName+'-CriterionData.xlsx'))\n",
    "\n",
    "saveDF = resultsDF.copy()\n",
    "del saveDF['data_peerGPT']\n",
    "saveDF.to_excel(os.path.join(excelFolder, saveName+'-ScoreData.xlsx'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Histogram spread of scores by Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "sns.set_theme(style=\"darkgrid\", palette=\"dark\")\n",
    "rcParams['figure.figsize'] = 5,3\n",
    "\n",
    "\n",
    "peerGPTGradesDF = saveDF.copy().drop(['score'], \n",
    "                                            axis=1, errors='ignore') \n",
    "peerGPTGradesDF = peerGPTGradesDF.rename(columns={'peerGPT_score':'score'})\n",
    "peerGPTGradesDF['Grader Type'] = 'peerGPT'\n",
    "\n",
    "gradersDF = saveDF.copy().drop(['peerGPT_score'], axis=1, errors='ignore') \n",
    "gradersDF['Grader Type'] = gradersDF['grader_id'].apply(lambda id: f'Grader ID: {id}')\n",
    "\n",
    "allGradesDF = pd.concat([gradersDF, peerGPTGradesDF])\n",
    "\n",
    "fig, axes = plt.subplots(nrows=len(saveDF['grader_id'].unique()), ncols=len(saveDF['assignment_id'].unique()), \\\n",
    "                       figsize=(18,9), layout=\"constrained\")\n",
    "\n",
    "for col, assignmentID in enumerate(saveDF['assignment_id'].unique()):\n",
    "    for row, graderID in enumerate(sorted(saveDF['grader_id'].unique())):\n",
    "        subsetDF = allGradesDF[(allGradesDF['assignment_id']==assignmentID) & (allGradesDF['grader_id']==graderID)]\n",
    "        # display(subsetDF)\n",
    "        upperX = int(subsetDF['points_possible'].iloc[0])\n",
    "        minScore = min(saveDF[(saveDF['assignment_id']==assignmentID)]['score'])\n",
    "        lowerX = int(minScore-minScore%2)\n",
    "        xTickStep = 1 if upperX-lowerX < 10 else 2\n",
    "\n",
    "        g = sns.histplot(ax=axes[row,col], data=subsetDF, x='score', hue='Grader Type', kde=True, multiple=\"dodge\", palette=sns.color_palette()[:2])\n",
    "        # g = sns.scatterplot(data=subsetDF, x='score', hue='Grader Type', palette=sns.color_palette()[:2])\n",
    "        g.set_xlim(lowerX,upperX)\n",
    "        g.set_xticks(range(lowerX,upperX+1, xTickStep))\n",
    "        # g.set_title(f'Assignment ID: {assignmentID}')\n",
    "        # g.set_xlabel('Points Awarded')\n",
    "        g.set_xlabel('Score Distribution', fontsize=8)\n",
    "        g.set_ylabel('')\n",
    "        g.legend([],[], frameon=False)\n",
    "\n",
    "pad = 5\n",
    "for ax, col in zip(axes[0], saveDF['assignment_id'].unique()):\n",
    "    ax.set_title(f'Assignment ID: {col}')\n",
    "for ax, row in zip(axes[:,0], sorted(saveDF['grader_id'].unique())):\n",
    "        ax.annotate(f'Grader ID: {row}', xy=(0, 0.5), xytext=(-ax.yaxis.labelpad - pad, 0),\n",
    "                    xycoords=ax.yaxis.label, textcoords='offset points',\n",
    "                    size='large', ha='right', va='center', rotation=90)\n",
    "\n",
    "# plt.show()\n",
    "plt.savefig(os.path.join(chartFolder, 'HistogramPlotSpread.png'), dpi=300, bbox_inches='tight')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Scatterplot spread of scores by Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "sns.set_theme(style=\"darkgrid\", palette=\"dark\")\n",
    "rcParams['figure.figsize'] = 4,4\n",
    "\n",
    "outlierFactor = 0.15\n",
    "\n",
    "gradesDF = saveDF.copy()\n",
    "gradesDF['Score Difference'] = gradesDF['peerGPT_score']-gradesDF['score']\n",
    "gradesDF['Outlier'] = (gradesDF['Score Difference']/(gradesDF['points_possible']*outlierFactor)).apply(lambda scoreDiff: 'Outlier' if abs(scoreDiff)>1 else 'In range')\n",
    "\n",
    "fig, axes = plt.subplots(nrows=len(saveDF['grader_id'].unique()), ncols=len(saveDF['assignment_id'].unique()), \\\n",
    "                       figsize=(15,10), layout=\"constrained\")\n",
    "\n",
    "for col, assignmentID in enumerate(saveDF['assignment_id'].unique()):\n",
    "    for row, graderID in enumerate(sorted(saveDF['grader_id'].unique())):\n",
    "        subsetDF = gradesDF[(gradesDF['assignment_id']==assignmentID) & (gradesDF['grader_id']==graderID)]\n",
    "        # display(subsetDF)\n",
    "        upperX = int(subsetDF['points_possible'].iloc[0])\n",
    "        minScore = min(saveDF[(saveDF['assignment_id']==assignmentID)]['score'])\n",
    "        lowerX = int(minScore-minScore%2)\n",
    "        xTickStep = 1 if upperX-lowerX < 10 else 2\n",
    "        shiftValue = int(subsetDF['points_possible'].iloc[0])*outlierFactor\n",
    "\n",
    "        g = sns.scatterplot(ax=axes[row,col], data=subsetDF, x='score', y='peerGPT_score', hue='Outlier', hue_order=gradesDF['Outlier'].unique(), palette=sns.color_palette()[:2])\n",
    "        axes[row,col].plot([lowerX,upperX],[lowerX,upperX], lw=1, color='#313232', linestyle='dashed')\n",
    "        axes[row,col].plot([lowerX+shiftValue,upperX+shiftValue],[lowerX,upperX], lw=1, color='#aaaaaa', linestyle='dashed')\n",
    "        axes[row,col].plot([lowerX,upperX],[lowerX+shiftValue,upperX+shiftValue], lw=1, color='#aaaaaa', linestyle='dashed')\n",
    "        g.set_xlim(lowerX,upperX+0.5)\n",
    "        g.set_xticks(range(lowerX,upperX+1, xTickStep))\n",
    "        g.set_ylim(lowerX,upperX+0.5)\n",
    "        g.set_yticks(range(lowerX,upperX+1, xTickStep))\n",
    "        # g.set_title(f'Assignment ID: {assignmentID},\\nGrader ID: {graderID}')\n",
    "        # g.set_xlabel('Human Grader Score')\n",
    "        # g.set_ylabel('peerGPT Score')\n",
    "        g.set_xlabel('Grader Score',fontsize=8)\n",
    "        g.set_ylabel('peerGPT Score', fontsize=8, rotation=90)\n",
    "        g.legend([],[], frameon=False)\n",
    "\n",
    "pad = 5\n",
    "for ax, col in zip(axes[0], saveDF['assignment_id'].unique()):\n",
    "    ax.set_title(f'Assignment ID: {col}')\n",
    "for ax, row in zip(axes[:,0], sorted(saveDF['grader_id'].unique())):\n",
    "        ax.annotate(f'Grader ID: {row}', xy=(0, 0.5), xytext=(-ax.yaxis.labelpad - pad, 0),\n",
    "                    xycoords=ax.yaxis.label, textcoords='offset points',\n",
    "                    size='large', ha='right', va='center', rotation=90)\n",
    "\n",
    "# plt.show()\n",
    "plt.savefig(os.path.join(chartFolder, 'ScatterPlotSpread.png'), dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Histogram spread of scores by Criterion per Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "import textwrap\n",
    "sns.set_theme(style=\"darkgrid\", palette=\"dark\")\n",
    "\n",
    "pad = 5\n",
    "\n",
    "saveFolder = os.path.join(chartFolder, 'histplots')\n",
    "if not os.path.exists(saveFolder):\n",
    "    os.mkdir(saveFolder)\n",
    "\n",
    "for col, assignmentID in enumerate(resultsDF['assignment_id'].unique()):\n",
    "\n",
    "    assgnDF = resultsDF[(resultsDF['assignment_id']==assignmentID)]\n",
    "    critDataDF = pd.DataFrame()\n",
    "    for index,row in assgnDF.iterrows():\n",
    "        criterionData = row['data_peerGPT']\n",
    "        for col in ['submitter_id', 'assignment_id', 'grader_id']:\n",
    "            criterionData[col] = row[col]\n",
    "        critDataDF = pd.concat([critDataDF, criterionData])\n",
    "\n",
    "    peerGPTGradesDF = critDataDF.copy().drop(['points_grade'], \n",
    "                                            axis=1, errors='ignore') \n",
    "    peerGPTGradesDF = peerGPTGradesDF.rename(columns={'peerGPT_criterion_score':'points_grade'})\n",
    "    peerGPTGradesDF['Grader Type'] = 'peerGPT'\n",
    "\n",
    "    gradersDF = critDataDF.copy().drop(['peerGPT_criterion_score'], \n",
    "                                                axis=1, errors='ignore') \n",
    "    gradersDF['Grader Type'] = gradersDF['grader_id'].apply(lambda id: f'Grader ID: {id}')\n",
    "\n",
    "    allCritDF = pd.concat([gradersDF, peerGPTGradesDF])\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=len(allCritDF['grader_id'].unique()), ncols=len(allCritDF['description_rubric'].unique()), \\\n",
    "                                        figsize=(len(allCritDF['description_rubric'].unique())*3,len(allCritDF['grader_id'].unique())*3), layout=\"constrained\")\n",
    "    fig.suptitle(f'Assignment ID: {assignmentID}')\n",
    "    \n",
    "    for col, descRubric in enumerate(sorted(allCritDF['description_rubric'].unique())):\n",
    "        for row, graderID in enumerate(sorted(allCritDF['grader_id'].unique())):\n",
    "            subsetDF = allCritDF[(allCritDF['grader_id']==graderID) & (allCritDF['description_rubric']==descRubric)].fillna(0)\n",
    "            # display(subsetDF)\n",
    "            upperX = int(subsetDF['points_rubric'].iloc[0])\n",
    "            lowerX = int(min(allCritDF[(allCritDF['description_rubric']==descRubric)]['points_grade']))\n",
    "            xTickStep = 1\n",
    "\n",
    "            try:\n",
    "                g = sns.histplot(ax=axes[row,col], data=subsetDF, x='points_grade', hue='Grader Type', kde=True, multiple=\"dodge\", palette=sns.color_palette()[:2])\n",
    "            except:\n",
    "                g = sns.histplot(ax=axes[row,col], data=subsetDF, x='points_grade', hue='Grader Type', kde=False, multiple=\"dodge\", palette=sns.color_palette()[:2])\n",
    "            g.set_xlim(lowerX,upperX)\n",
    "            g.set_xticks(range(lowerX,upperX+1, xTickStep))\n",
    "            # g.set_title(f'Assignment ID: {assignmentID}')\n",
    "            # g.set_xlabel('Points Awarded')\n",
    "            g.set_xlabel('Score Distribution', fontsize=8)\n",
    "            g.set_ylabel('')\n",
    "            g.legend([],[], frameon=False)\n",
    "\n",
    "    for ax, col in zip(axes[0], sorted(allCritDF['description_rubric'].unique())):\n",
    "        newLine = '\\n'.join(textwrap.wrap(col, width=24))\n",
    "        ax.set_title(f'{newLine}')\n",
    "    # for ax, row in zip(axes[:,0], sorted(allCritDF['grader_id'].unique())):\n",
    "    #     ax.set_ylabel(f'Grader ID: {row}', rotation=90)\n",
    "    for ax, row in zip(axes[:,0], sorted(allCritDF['grader_id'].unique())):\n",
    "        ax.annotate(f'Grader ID: {row}', xy=(0, 0.5), xytext=(-ax.yaxis.labelpad - pad, 0),\n",
    "                    xycoords=ax.yaxis.label, textcoords='offset points',\n",
    "                    size='large', ha='right', va='center', rotation=90)\n",
    "\n",
    "    # plt.show()\n",
    "    # break\n",
    "    # plt.savefig(os.path.join(saveFolder, f'{assignmentID}-HistogramPlotSpread.png'), dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Scatterplot spread of scores by Criterion per Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "import textwrap\n",
    "sns.set_theme(style=\"darkgrid\", palette=\"dark\")\n",
    "\n",
    "outlierFactor = 0.15\n",
    "pad = 5\n",
    "\n",
    "saveFolder = os.path.join(chartFolder, 'scatterplots')\n",
    "if not os.path.exists(saveFolder):\n",
    "    os.mkdir(saveFolder)\n",
    "\n",
    "\n",
    "for col, assignmentID in enumerate(resultsDF['assignment_id'].unique()):\n",
    "\n",
    "    assgnDF = resultsDF[(resultsDF['assignment_id']==assignmentID)]\n",
    "    critDataDF = pd.DataFrame()\n",
    "    for index,row in assgnDF.iterrows():\n",
    "        criterionData = row['data_peerGPT']\n",
    "        for col in ['submitter_id', 'assignment_id', 'grader_id']:\n",
    "            criterionData[col] = row[col]\n",
    "        critDataDF = pd.concat([critDataDF, criterionData])\n",
    "\n",
    "    allCritDF = critDataDF.copy()\n",
    "    allCritDF['Score Difference'] = allCritDF['peerGPT_criterion_score']-allCritDF['points_grade']\n",
    "    allCritDF['Outlier'] = (allCritDF['Score Difference']/(allCritDF['points_rubric']*outlierFactor)).apply(lambda scoreDiff: 'Outlier' if abs(scoreDiff)>1 else 'In range')\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=len(allCritDF['grader_id'].unique()), ncols=len(allCritDF['description_rubric'].unique()), \\\n",
    "                                        figsize=(len(allCritDF['description_rubric'].unique())*3,len(allCritDF['grader_id'].unique())*3), layout=\"constrained\")\n",
    "    fig.suptitle(f'Assignment ID: {assignmentID}')\n",
    "    \n",
    "    for col, descRubric in enumerate(sorted(allCritDF['description_rubric'].unique())):\n",
    "        for row, graderID in enumerate(sorted(allCritDF['grader_id'].unique())):\n",
    "            subsetDF = allCritDF[(allCritDF['assignment_id']==assignmentID) & (allCritDF['grader_id']==graderID)]\n",
    "            # display(subsetDF)\n",
    "            upperX = int(subsetDF['points_rubric'].iloc[0])+1\n",
    "            lowerX = int(min(allCritDF[(allCritDF['assignment_id']==assignmentID)]['points_grade']))\n",
    "            xTickStep = 1\n",
    "            shiftValue = int(subsetDF['points_rubric'].iloc[0])*outlierFactor\n",
    "\n",
    "            g = sns.scatterplot(ax=axes[row,col], data=subsetDF, x='points_grade', y='peerGPT_criterion_score', hue='Outlier', hue_order=allCritDF['Outlier'].unique(), palette=sns.color_palette()[:2])\n",
    "            axes[row,col].plot([lowerX,upperX],[lowerX,upperX], lw=1, color='#313232', linestyle='dashed')\n",
    "            axes[row,col].plot([lowerX+shiftValue,upperX+shiftValue],[lowerX,upperX], lw=1, color='#aaaaaa', linestyle='dashed')\n",
    "            axes[row,col].plot([lowerX,upperX],[lowerX+shiftValue,upperX+shiftValue], lw=1, color='#aaaaaa', linestyle='dashed')\n",
    "            g.set_xlim(lowerX-0.1,upperX+0.1)\n",
    "            g.set_xticks(range(lowerX,upperX+1, xTickStep))\n",
    "            g.set_ylim(lowerX-0.1,upperX+0.1)\n",
    "            g.set_yticks(range(lowerX,upperX+1, xTickStep))\n",
    "            # g.set_title(f'Assignment ID: {assignmentID},\\nGrader ID: {graderID}')\n",
    "            # g.set_xlabel('Human Grader Score')\n",
    "            # g.set_ylabel('peerGPT Score')\n",
    "            g.set_xlabel('Grader Score',fontsize=8)\n",
    "            g.set_ylabel('peerGPT Score', fontsize=8, rotation=90)\n",
    "            g.legend([],[], frameon=False)\n",
    "\n",
    "    for ax, col in zip(axes[0], sorted(allCritDF['description_rubric'].unique())):\n",
    "        newLine = '\\n'.join(textwrap.wrap(col, width=24))\n",
    "        ax.set_title(f'{newLine}')\n",
    "    # for ax, row in zip(axes[:,0], sorted(allCritDF['grader_id'].unique())):\n",
    "    #     ax.set_ylabel(, rotation=90)\n",
    "    for ax, row in zip(axes[:,0], sorted(allCritDF['grader_id'].unique())):\n",
    "        ax.annotate(f'Grader ID: {row}', xy=(0, 0.5), xytext=(-ax.yaxis.labelpad - pad, 0),\n",
    "                    xycoords=ax.yaxis.label, textcoords='offset points',\n",
    "                    size='large', ha='right', va='center', rotation=90)\n",
    "\n",
    "    # plt.show()\n",
    "    # break\n",
    "    # plt.savefig(os.path.join(saveFolder, f'{assignmentID}-ScatterPlotSpread.png'), dpi=300, bbox_inches='tight')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Track Difference in scores per Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveDF['Score Difference'] = saveDF['peerGPT_score']-saveDF['score']\n",
    "excludeDF = saveDF\n",
    "\n",
    "meanDiffDF = {}\n",
    "for group in excludeDF.groupby(['grader_id','assignment_id']):\n",
    "    if group[0][0] not in meanDiffDF:\n",
    "        meanDiffDF[group[0][0]] = {}\n",
    "    meanDiffDF[group[0][0]][group[0][1]] = group[1][\"Score Difference\"].mean()\n",
    "\n",
    "pd.DataFrame(meanDiffDF).to_excel(os.path.join(excelFolder, saveName+' - Grader - peerGPT Score Difference.xlsx'))\n",
    "pd.DataFrame(meanDiffDF)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Track difference in score per Criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "critDataDF = pd.DataFrame()\n",
    "for index,row in resultsDF.iterrows():\n",
    "    criterionData = row['data_peerGPT']\n",
    "    for col in ['submitter_id', 'assignment_id', 'grader_id']:\n",
    "        criterionData[col] = row[col]\n",
    "    critDataDF = pd.concat([critDataDF, criterionData])\n",
    "allCritDF = critDataDF.drop(['mastery_points','ignore_for_scoring','title','peerGPT_criterion_id','description_grade'],\n",
    "                           axis=1, errors='ignore')\n",
    "\n",
    "meanInfoList = []\n",
    "for group in allCritDF.groupby(['assignment_id','criterion_id','grader_id']):\n",
    "    meanInfoList.append({'assignment_id':group[0][0], 'criterion_id':group[0][1], 'grader_id':group[0][2], \\\n",
    "                         'Grader Mean':group[1]['points_grade'].mean(), \\\n",
    "                         'Grader Std. Dev.':group[1]['points_grade'].std(), \\\n",
    "                         'peerGPT Mean':group[1]['peerGPT_criterion_score'].mean(), \\\n",
    "                         'peerGPT Std. Dev.':group[1]['peerGPT_criterion_score'].std(), \\\n",
    "                         'Correlation Score':group[1]['peerGPT_criterion_score'].corr(group[1]['points_grade'])})\n",
    "meanInfoDF = pd.DataFrame(meanInfoList)\n",
    "meanInfoDF['Mean Difference'] = meanInfoDF['peerGPT Mean'] - meanInfoDF['Grader Mean']\n",
    "\n",
    "assignmentDF = getGRAData(config)[['assignment_id', 'assignment_title']].drop_duplicates()\n",
    "baseInfoDF = allCritDF[['assignment_id', 'criterion_id', 'description_rubric', 'points_rubric']].drop_duplicates()\n",
    "baseInfoDF = baseInfoDF.merge(assignmentDF, on='assignment_id')\n",
    "\n",
    "globalMeanList = [{'assignment_id':group[0][0], 'criterion_id':group[0][1], \\\n",
    "                  'All Graders Mean':group[1]['points_grade'].mean(), \\\n",
    "                  'All Graders Std. Dev.':group[1]['points_grade'].std(), \\\n",
    "                  'Global peerGPT Mean':group[1]['peerGPT_criterion_score'].mean(), \\\n",
    "                  'Global peerGPT Std. Dev.':group[1]['peerGPT_criterion_score'].std()} \\\n",
    "                    for group in allCritDF.groupby(['assignment_id', 'criterion_id'])]\n",
    "globalMeanDF = pd.DataFrame(globalMeanList)\n",
    "baseInfoDF = baseInfoDF.merge(globalMeanDF, on=['assignment_id', 'criterion_id'])\n",
    "\n",
    "fullInfoDF = meanInfoDF.merge(baseInfoDF, on=['assignment_id', 'criterion_id'])\n",
    "fullInfoDF['Mean Difference %'] = 100*fullInfoDF['Mean Difference'].div(fullInfoDF['points_rubric']) \n",
    "\n",
    "fullInfoDF.to_excel(os.path.join(excelFolder, saveName+' - Grader Difference Table.xlsx'))\n",
    "\n",
    "rubricInfo = getGRAData(config)[['assignment_id', 'data_rubric']].drop_duplicates('assignment_id').reset_index(drop=True)\n",
    "rubricOrderDict = {}\n",
    "for index, row in rubricInfo.iterrows():\n",
    "    rubricOrderDict[row['assignment_id']] = pd.DataFrame(row['data_rubric'])['description'].tolist()\n",
    "\n",
    "fullInfoDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import textwrap\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "sns.set_theme(style=\"darkgrid\") #, palette=\"dark\")\n",
    "\n",
    "versionControl = 'V3'\n",
    "promptVersion='P2'\n",
    "\n",
    "saveName = f\"{versionControl}-{promptVersion}\"\n",
    "saveMeanFolder = os.path.join(chartFolder, 'Mean Difference Charts')\n",
    "if not os.path.exists(saveMeanFolder):\n",
    "    os.mkdir(saveMeanFolder)\n",
    "\n",
    "for AID in fullInfoDF['assignment_id'].unique():\n",
    "    subsetDF =  fullInfoDF[fullInfoDF['assignment_id']==AID]\n",
    "    plt.clf() \n",
    "\n",
    "    upperY = math.ceil(max(subsetDF['Mean Difference'])*10)/10\n",
    "    lowerY = math.floor(min(subsetDF['Mean Difference'])*10)/10\n",
    "\n",
    "    if upperY<0:\n",
    "        upperY = 0\n",
    "\n",
    "    if upperY-lowerY>1.6:\n",
    "        tickStep = 0.2\n",
    "    else:\n",
    "        tickStep = 0.1\n",
    "\n",
    "    if lowerY<0 and upperY>0:\n",
    "        tickSpace = np.concatenate((np.arange(lowerY-lowerY%tickStep,0, tickStep),np.arange(0,upperY+upperY%tickStep+0.1, tickStep)))\n",
    "    else:\n",
    "        tickSpace = np.arange(lowerY,upperY+0.1, tickStep)\n",
    "\n",
    "    sns.set(rc={'figure.figsize':((3/2)*len(rubricOrderDict[AID]),3)})\n",
    "\n",
    "    g = sns.stripplot(data=subsetDF, x='description_rubric', y='Mean Difference', \\\n",
    "                        order = rubricOrderDict[AID], \\\n",
    "                        hue='grader_id', dodge=False, jitter=True, \\\n",
    "                        palette=sns.color_palette()[:4])\n",
    "    \n",
    "    plt.axhline(y=0, color='#313232', linestyle='--')\n",
    "\n",
    "    g.set_ylim(lowerY-0.05,upperY+0.05)\n",
    "    g.set_yticks(tickSpace)\n",
    "    g.set_xlabel('Criteria', fontsize=12)\n",
    "    g.set_ylabel('Mean Difference', fontsize=12, rotation=90)\n",
    "\n",
    "    g.set_xticks(g.get_xticks())\n",
    "\n",
    "    wrapSize = 14 if len(rubricOrderDict[AID]) < 6 else 12 \n",
    "    g.set_xticklabels([textwrap.fill(t.get_text(), wrapSize, break_long_words=False) for t in g.get_xticklabels()], size=9)\n",
    "\n",
    "    g.set_title(textwrap.fill(subsetDF['assignment_title'].iloc[0], 50))\n",
    "\n",
    "    plt.legend(bbox_to_anchor=(1.01, 1), loc='upper left', borderaxespad=0, title='Grader ID')\n",
    "    # plt.show()\n",
    "    # g.get_figure().savefig(os.path.join(saveMeanFolder, f'{AID}-MeanDiffSpread.png'), dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import textwrap\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "sns.set_theme(style=\"darkgrid\") #, palette=\"dark\")\n",
    "\n",
    "versionControl = 'V3'\n",
    "promptVersion='P2'\n",
    "\n",
    "saveName = f\"{versionControl}-{promptVersion}\"\n",
    "saveMeanFolder = os.path.join(chartFolder, 'Mean Diff %')\n",
    "if not os.path.exists(saveMeanFolder):\n",
    "    os.mkdir(saveMeanFolder)\n",
    "\n",
    "for AID in fullInfoDF['assignment_id'].unique():\n",
    "    subsetDF =  fullInfoDF[fullInfoDF['assignment_id']==AID]\n",
    "    plt.clf() \n",
    "\n",
    "    upperY = math.ceil(max(subsetDF['Mean Difference %']))\n",
    "    lowerY = math.floor(min(subsetDF['Mean Difference %']))\n",
    "\n",
    "    if upperY<0:\n",
    "        upperY = 0\n",
    "    if lowerY>0:\n",
    "        lowerY = 0\n",
    "\n",
    "    if upperY-lowerY>160:\n",
    "        tickStep = 10\n",
    "    else:\n",
    "        tickStep = 5\n",
    "\n",
    "    if lowerY<0 and upperY>0:\n",
    "        tickSpace = np.concatenate((np.arange(lowerY-lowerY%tickStep,0, tickStep),np.arange(0,upperY+upperY%tickStep+5, tickStep)))\n",
    "    else:\n",
    "        tickSpace = np.arange(lowerY,upperY+10, tickStep)\n",
    "\n",
    "    sns.set(rc={'figure.figsize':((3/2)*len(rubricOrderDict[AID]),3)})\n",
    "\n",
    "    g = sns.stripplot(data=subsetDF, x='description_rubric', y='Mean Difference %', \\\n",
    "                        order = rubricOrderDict[AID], \\\n",
    "                        hue='grader_id', dodge=False, jitter=True, \\\n",
    "                        palette=sns.color_palette()[:4])\n",
    "    \n",
    "    plt.axhline(y=0, color='#313232', linestyle='--')\n",
    "\n",
    "    g.set_ylim(lowerY-5,upperY+5)\n",
    "    g.set_yticks(tickSpace)\n",
    "    g.set_xlabel('Criteria', fontsize=12)\n",
    "    g.set_ylabel('Mean Difference %', fontsize=12, rotation=90)\n",
    "\n",
    "    g.set_xticks(g.get_xticks())\n",
    "\n",
    "    wrapSize = 14 if len(rubricOrderDict[AID]) < 6 else 12 \n",
    "    g.set_xticklabels([textwrap.fill(t.get_text(), wrapSize, break_long_words=False) for t in g.get_xticklabels()], size=9)\n",
    "\n",
    "    g.set_title(textwrap.fill(subsetDF['assignment_title'].iloc[0], 50))\n",
    "\n",
    "    plt.legend(bbox_to_anchor=(1.01, 1), loc='upper left', borderaxespad=0, title='Grader ID')\n",
    "    plt.show()\n",
    "    # g.get_figure().savefig(os.path.join(saveMeanFolder, f'{AID}-MeanDiffSpread.png'), dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats\n",
    "\n",
    "def confindenceInterval(data, confidence=0.9):\n",
    "    a = 1.0 * np.array(data)\n",
    "    n = len(a)\n",
    "    m, se = np.mean(a), scipy.stats.sem(a)\n",
    "    h = se * scipy.stats.t.ppf((1 + confidence) / 2., n-1)\n",
    "    return m-h, m+h\n",
    "\n",
    "confidenceList = np.arange(0.85,0.99, 0.01)\n",
    "\n",
    "confidenceDataDict = {}\n",
    "\n",
    "for confidence in confidenceList:\n",
    "    outsideCIDF = pd.DataFrame()\n",
    "    for AID in fullInfoDF['assignment_id'].unique():\n",
    "        for CID in fullInfoDF[(fullInfoDF['assignment_id']==AID)]['criterion_id'].unique(): \n",
    "            subsetDF =  fullInfoDF[(fullInfoDF['assignment_id']==AID) & (fullInfoDF['criterion_id']==CID)]\n",
    "            meanDiffDict = dict(zip(subsetDF['grader_id'].tolist(),subsetDF['Mean Difference %'].tolist()))\n",
    "            lower,upper = confindenceInterval(list(meanDiffDict.values()), confidence)\n",
    "            for grader in meanDiffDict:\n",
    "                if meanDiffDict[grader]<lower or meanDiffDict[grader]>upper:\n",
    "                    # display(subsetDF[subsetDF['grader_id']==grader])\n",
    "                    outsideCIDF = pd.concat([outsideCIDF,subsetDF[subsetDF['grader_id']==grader]])\n",
    "\n",
    "    # print(outsideCIDF.shape)\n",
    "    if confidence==0.93:\n",
    "        display(outsideCIDF)\n",
    "    if not outsideCIDF.empty:\n",
    "        confidenceDataDict[np.round(confidence, 2)] = outsideCIDF['grader_id'].value_counts().to_dict()\n",
    "    else:\n",
    "        break\n",
    "\n",
    "pd.DataFrame(confidenceDataDict).sort_index()#.to_excel(os.path.join(excelFolder, saveName+' - Confidence in Mean Diff % Table.xlsx'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.precision', 2)\n",
    "\n",
    "criteriaIssuesDF = pd.DataFrame()\n",
    "scoreThreshold = 10 # In percent\n",
    "\n",
    "for AID in fullInfoDF['assignment_id'].unique():\n",
    "    for CID in fullInfoDF[(fullInfoDF['assignment_id']==AID)]['criterion_id'].unique(): \n",
    "        subsetDF =  fullInfoDF[(fullInfoDF['assignment_id']==AID) & (fullInfoDF['criterion_id']==CID)]\n",
    "        if subsetDF['Mean Difference %'].mean() > scoreThreshold:\n",
    "            issueDF = subsetDF[['assignment_id', 'criterion_id', 'assignment_title', 'description_rubric', 'points_rubric', 'All Graders Mean', 'Global peerGPT Mean']].drop_duplicates()\n",
    "            issueDF['Mean Difference %'] = subsetDF['Mean Difference %'].mean()\n",
    "            criteriaIssuesDF = pd.concat([criteriaIssuesDF, issueDF])\n",
    "\n",
    "criteriaIssuesDF = criteriaIssuesDF.rename(columns={'assignment_title':'Title', 'description_rubric':'Rubric', 'points_rubric':'Max Score'}).reset_index(drop=True)\n",
    "criteriaIssuesDF.to_excel(os.path.join(excelFolder, saveName+' - High Error Criteria.xlsx'))\n",
    "\n",
    "criteriaIssuesDF"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Misc. Code Snippets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import *\n",
    "\n",
    "config = Config()\n",
    "config.setFromEnv()\n",
    "\n",
    "versionControl = 'V2'\n",
    "promptVersion='P1'\n",
    "fullName = f'{versionControl}-{promptVersion}'\n",
    "config.setSaveDetails(fullName)\n",
    "\n",
    "config.simpleCourseName = 'Movement Science'\n",
    "\n",
    "saveName = f\"{versionControl}-{promptVersion}\"\n",
    "\n",
    "\n",
    "gradeRubricAssignmentDF = getGRAData(config)\n",
    "\n",
    "for index, row in gradeRubricAssignmentDF.iterrows():\n",
    "    gradeDataDF = pd.DataFrame(row['data_grade'])\n",
    "    rubricDataDF = pd.DataFrame(row['data_rubric'])\n",
    "    descDataDF = config.critDescDF[config.critDescDF['assignment_id']==row['assignment_id']][['custom_description', 'id']]\n",
    "\n",
    "    fullCriterionDF = gradeDataDF.merge(rubricDataDF, left_on='criterion_id', \n",
    "                                        right_on='id', suffixes=('_grade', '_rubric'))\n",
    "    fullCriterionDF = fullCriterionDF.merge(descDataDF, left_on='criterion_id', right_on='id')\n",
    "    fullCriterionDF = fullCriterionDF.drop(['id_grade', 'id_rubric', 'learning_outcome_id', 'id',\n",
    "                                            'comments_enabled', 'comments_html', 'criterion_use_range'], \n",
    "                                            axis=1, errors='ignore')    \n",
    "\n",
    "    fullCritText = buildCritPrompt(fullCriterionDF, True)\n",
    "    studentSubmission = getSubmissionText(row['assignment_id'], row['submitter_id'])\n",
    "\n",
    "    if studentSubmission:\n",
    "        promptVariableDict = {\n",
    "                            'Course Name': config.simpleCourseName,\n",
    "                            'Assignment Name': row['assignment_title'],\n",
    "                            'Assignment Description': row['cleaned_description'],\n",
    "                            'Student Submission': studentSubmission,\n",
    "                            'Criterion Description and Rubric': fullCritText,\n",
    "                            'Maximum Points': row['points_possible'],\n",
    "                            }\n",
    "        fullPrompt = promptBuilder(promptVariableDict)\n",
    "\n",
    "        # print(fullPrompt)\n",
    "        with open(os.path.join(config.versionOutputFolder, f'{config.fullName}_exampleFilledPrompt.txt'), 'w') as textFile:\n",
    "            textFile.write(fullPrompt)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errorDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
