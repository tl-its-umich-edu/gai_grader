{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import *\n",
    "from postProcessHelper import *\n",
    "\n",
    "config = Config()\n",
    "config.setFromEnv()\n",
    "\n",
    "versionControl = 'V3'\n",
    "promptVersion='P2'\n",
    "saveName = f\"{versionControl}-{promptVersion}\"\n",
    "\n",
    "config.setSaveDetails(saveName)\n",
    "\n",
    "chartFolder = os.path.join(config.baseOutputFolder, \\\n",
    "                           config.outputFolders['CHART_OUTPUT'], \\\n",
    "                           config.fullName)\n",
    "\n",
    "excelFolder = os.path.join(config.baseOutputFolder, \\\n",
    "                           config.outputFolders['EXCEL_OUTPUT'], \\\n",
    "                           config.fullName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultsDF = convertPicklesToDF('saves', config)\n",
    "errorDF = convertPicklesToDF('errors', config)\n",
    "\n",
    "mergedCriterionData = getCriterionDataDF(resultsDF, saveName, excelFolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getScoreSpread(resultsDF, chartFolder)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Histogram spread of scores by Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getFullHistogramSpread(resultsDF, chartFolder)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Scatterplot spread of scores by Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getFullScatterplotSpread(resultsDF, chartFolder)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Histogram spread of scores by Criterion per Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getHistogramSpreadByAssgn(resultsDF, chartFolder)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Scatterplot spread of scores by Criterion per Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getScatterplotSpreadByAssgn(resultsDF, chartFolder)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Track Difference in scores per Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveGraderPeerGPTMeanScoreDiff(resultsDF, saveName, excelFolder)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Track difference in score per Criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullInfoDF, rubricOrderDict = buildFullInfoDF(config, resultsDF, saveName, excelFolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getMeanDiffCharts(fullInfoDF, rubricOrderDict, chartFolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getMeanDiffPercentCharts(fullInfoDF, rubricOrderDict, chartFolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meanConDF, retrievedCIDF = getCIOutlierGraderDetails(fullInfoDF, saveName, excelFolder, confidenceLevel=0.93)\n",
    "\n",
    "display(retrievedCIDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getHighErrorCriteria(config, fullInfoDF, saveName, excelFolder, scoreThreshold = 10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Misc. Code Snippets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import *\n",
    "\n",
    "config = Config()\n",
    "config.setFromEnv()\n",
    "config.customDescMode = True\n",
    "\n",
    "versionControl = 'V3'\n",
    "promptVersion='P2'\n",
    "fullName = f'{versionControl}-{promptVersion}'\n",
    "config.setSaveDetails(fullName)\n",
    "\n",
    "saveName = f\"{versionControl}-{promptVersion}\"\n",
    "\n",
    "\n",
    "gradeRubricAssignmentDF = getGRAData(config)\n",
    "\n",
    "for index, row in gradeRubricAssignmentDF.iterrows():\n",
    "    fullCriterionDF = getRowCriterionDF(row, config)\n",
    "    fullCritText = buildCritPrompt(fullCriterionDF, useCustomDesc=config.customDescMode)\n",
    "    studentSubmission = getSubmissionText(row['assignment_id'], row['submitter_id'], config)\n",
    "\n",
    "    if studentSubmission:\n",
    "        promptVariableDict = {\n",
    "                            'Course Name': config.simpleCourseName,\n",
    "                            'Assignment Name': row['assignment_title'],\n",
    "                            'Assignment Description': row['cleaned_description'],\n",
    "                            'Student Submission': studentSubmission,\n",
    "                            'Criterion Description and Rubric': fullCritText,\n",
    "                            'Maximum Points': row['points_possible'],\n",
    "                            }\n",
    "        fullPrompt = promptBuilder(promptVariableDict, useCustomDesc=config.customDescMode)\n",
    "\n",
    "        # print(fullPrompt)\n",
    "\n",
    "        with open(os.path.join(config.baseOutputFolder, \\\n",
    "                               config.outputFolders['PROMPT_FILES'], \\\n",
    "                               config.fullName, \\\n",
    "                               f'{config.fullName}_exampleFilledPrompt.txt'), 'w') as textFile:\n",
    "            textFile.write(fullPrompt)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errorDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
