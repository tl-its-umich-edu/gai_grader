{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis and generation of Tables and Charts for graded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import *\n",
    "from postProcessHelper import *\n",
    "\n",
    "config = Config()\n",
    "config.setFromEnv()\n",
    "\n",
    "# Option to use custom variables here.\n",
    "versionControl = 'V3'\n",
    "promptVersion = 'P2'\n",
    "courseShorthand = 'MOVESCI'\n",
    "\n",
    "customConfigParams = {\n",
    "                'Save Name':f'{courseShorthand}-{versionControl}-{promptVersion}', \n",
    "                'Overwrite Saves': False, \n",
    "                'Use Custom Desc.': True\n",
    "                }\n",
    "config.setSaveDetails(customConfigParams)\n",
    "\n",
    "config.setSaveDetails()\n",
    "\n",
    "chartFolder = os.path.join(config.baseOutputFolder, \\\n",
    "                           config.outputFolders['CHART_OUTPUT'], \\\n",
    "                           config.fullName)\n",
    "\n",
    "excelFolder = os.path.join(config.baseOutputFolder, \\\n",
    "                           config.outputFolders['EXCEL_OUTPUT'], \\\n",
    "                           config.fullName)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultsDF = convertPicklesToDF('saves', config)\n",
    "errorDF = convertPicklesToDF('errors', config)\n",
    "\n",
    "mergedCriterionData = getCriterionDataDF(resultsDF, config.fullName, excelFolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getScoreSpread(resultsDF, chartFolder)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Track Difference in scores per Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meanDiffDF, meanDiffPercentDF = saveGraderPeerGPTMeanScoreDiff(resultsDF, config.fullName, excelFolder)\n",
    "\n",
    "display(meanDiffPercentDF)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Track difference in score per Criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullInfoDF, rubricOrderDict = buildFullInfoDF(config, resultsDF, config.fullName, excelFolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getZScoreAndCI(fullInfoDF, config.fullName, excelFolder, confidence=0.93)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meanConDF, retrievedCIDF = getCIOutlierGraderDetails(fullInfoDF, config.fullName, excelFolder, confidenceLevel=0.93, testType='graderGPT')\n",
    "\n",
    "display(meanConDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getMeanDiffPercentCharts(fullInfoDF, rubricOrderDict, chartFolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getHighErrorCriteria(config, fullInfoDF, config.fullName, excelFolder, scoreThreshold = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The following functions works, but are not necesarry for analysis currently. \n",
    "They also have issues regarding layout and formatting of charts, and will need to be manually fixed in the `postProcessHelper.py` script."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Histogram spread of scores by Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getFullHistogramSpread(resultsDF, chartFolder)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Scatterplot spread of scores by Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getFullScatterplotSpread(resultsDF, chartFolder)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Histogram spread of scores by Criterion per Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getHistogramSpreadByAssgn(resultsDF, chartFolder)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Scatterplot spread of scores by Criterion per Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getScatterplotSpreadByAssgn(resultsDF, chartFolder)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Misc. Code Snippets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getMeanDiffCharts(fullInfoDF, rubricOrderDict, chartFolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
